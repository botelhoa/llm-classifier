{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3cd6fb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b27007",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "598a0728",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_classifier.utils.model import LLMCLassifier, CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3f3e3d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instantiating an MPTForCausalLM model from /home/abotelho/.cache/huggingface/modules/transformers_modules/mosaicml/mpt-7b-instruct/1fc4634127ec64a45716003578b9cfae23265849/modeling_mpt.py\n",
      "You are using config.init_device='cpu', but you can also use config.init_device=\"meta\" with Composer + FSDP for fast initialization.\n",
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /home/abotelho/.cache/pypoetry/virtualenvs/llm-classifier-wLs50JxH-py3.11/lib/python3.11/site-packages/bitsandbytes/libbitsandbytes_cuda113.so\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so.11.0\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.0\n",
      "CUDA SETUP: Detected CUDA version 113\n",
      "CUDA SETUP: Loading binary /home/abotelho/.cache/pypoetry/virtualenvs/llm-classifier-wLs50JxH-py3.11/lib/python3.11/site-packages/bitsandbytes/libbitsandbytes_cuda113.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abotelho/.cache/pypoetry/virtualenvs/llm-classifier-wLs50JxH-py3.11/lib/python3.11/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /opt/conda did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4707ba1783124cb9885bd8129e27dfd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n",
      "The model 'MPTForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cuda:0\n",
      "Pipe type:  text-generation\n"
     ]
    }
   ],
   "source": [
    "model = LLMCLassifier(\n",
    "            model_path= 'models/TheBloke_stable-vicuna-13B-HF', \n",
    "            config=CONFIG, \n",
    "            candidate_labels= [],\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c475a1d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "output = model.run(\"Explain to me the difference between nuclear fission and fusion.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "610db5df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nNuclear Fission is a process that splits heavy atoms into smaller, lighter ones by releasing energy in the form of heat or radiation (gamma rays'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "762b0637",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_classifier.utils.data_handling import task_maps\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "\n",
    "def clean_prediction(pred, task: str, missing_val: int):\n",
    "\n",
    "    if pd.isna(pred) == False:\n",
    "        return task_maps[task].get(pred.split(\": \")[-1], missing_val)\n",
    "    else:\n",
    "        return missing_val\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "data_dir = os.path.join(os.getcwd().split(\"llm_classifier\")[0], \"llm_classifier/data\")\n",
    "task = \"topic\"\n",
    "\n",
    "df = pd.read_parquet(f\"{data_dir}/{task}/{task}_sample.parquet\")\n",
    "\n",
    "id_col = \"id\" if task==\"topic\" else \"account_id\"\n",
    "\n",
    "df = df.rename(columns={\"bias\": \"labels\", \"trustworthy\": \"labels\", \"topic\": \"labels\", \"final_text\": \"text\"})\n",
    "df[\"labels_numeric\"] = df[\"labels\"].map(task_maps[task])\n",
    "\n",
    "\n",
    "temp_df = pd.read_csv(\"/home/abotelho/llm-classifier/llm_classifier/data/topic/TheBloke_stable-vicuna-13B-HF/topic_zero_predictions.csv\")\n",
    "temp_df[\"id\"] = temp_df[\"id\"].astype(str)\n",
    "\n",
    "\n",
    "missing_val = task_maps[task][\"miscellaneous\"] if task == \"topic\" else task_maps[task][\"moderate\"] if task == \"partisanship\" else 0\n",
    "temp_df[\"prediction_numeric\"] = temp_df[\"prediction\"].map(lambda x: clean_prediction(x, task, missing_val))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ed04e274",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = df.merge(temp_df, left_on=id_col, right_on=\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dd4b1404",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(58, 36)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3b8f5121",
   "metadata": {},
   "outputs": [],
   "source": [
    "prec, rec, f1, _ = precision_recall_fscore_support(temp_df[\"labels_numeric\"], temp_df[\"prediction_numeric\"], average=\"weighted\", zero_division=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "94fee679",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09195402298850575"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
